{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import asyncio\n",
    "import pathlib\n",
    "import os\n",
    "import logging\n",
    "import aiohttp\n",
    "import requests\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp.client_exceptions as aio_cl_err\n",
    "\n",
    "DOCUMENT_ROOT = 'saved_article'\n",
    "URL = 'https://news.ycombinator.com/'\n",
    "TIMEOUT = 10\n",
    "HANDLE_INTERVAL = 1\n",
    "CHECK_INTERVAL = 120\n",
    "MAX_RATE = 30\n",
    "\n",
    "class FetchTask:\n",
    "    \"\"\"\n",
    "    Класс для загрузки данных со страницы\n",
    "    и сохранения их в указанной директории\n",
    "    (под каждую страницу создается объект данного класса)\n",
    "    \"\"\"\n",
    "    def __init__(self, id_link, result_dir):\n",
    "        self.id = id_link[0]\n",
    "        self.url = id_link[1]\n",
    "        self.result_dir = result_dir\n",
    "\n",
    "    def save(self, data, name):\n",
    "        \"\"\"\n",
    "        Метод сохранения спарсенных данных\n",
    "\n",
    "        :param data: данные для сохранения\n",
    "        :param name: имя файла\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.result_dir+'/'+self.id, exist_ok=True)\n",
    "            with open(self.result_dir+'/'+self.id+'/'+name+'.html', 'wb') as file:\n",
    "                file.write(data)\n",
    "        except OSError:\n",
    "            logging.error('Can\\'t save file')\n",
    "\n",
    "    def get_name(self):\n",
    "        \"\"\"\n",
    "        :return: имя файла для сохранения данных со страницы\n",
    "        \"\"\"\n",
    "        return self.url.strip('/').split('/')[-1]\n",
    "\n",
    "    async def perform(self):\n",
    "        \"\"\"\n",
    "        Метод для асинхронного получения данных с указанной страницы\n",
    "\n",
    "        \"\"\"\n",
    "        logging.debug('downloading url: {}'.format(self.url))\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                async with session.get(self.url) as resp:\n",
    "                    data = await resp.read()\n",
    "                    name = self.get_name()\n",
    "                    await asyncio.get_running_loop().run_in_executor(\n",
    "                        None, self.save, data, name)\n",
    "            except (aio_cl_err.ClientConnectorError, aio_cl_err.InvalidURL):\n",
    "                logging.error('Can\\'t parse link: {}'.format(self.url))\n",
    "\n",
    "\n",
    "class Pool:\n",
    "    \"\"\"\n",
    "    Класс, предназначенный для управления количеством запросов к ресурсу\n",
    "    за еденицу времени (решение использовать данный класс пришло в связи\n",
    "    с тем, что бывают ресурсы, которые ограничивают кол-во однотипных запросов,\n",
    "    а данный класс позволит избежать данных блокировок без применения Selenium)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_rate, interval=HANDLE_INTERVAL):\n",
    "        self.max_rate = max_rate # максимальное количество запросов\n",
    "        self.interval = interval # интервал запросов\n",
    "        self.is_running = False # флаг работы\n",
    "        self.queue = asyncio.Queue() # очередь задач на выполнение\n",
    "        self._scheduler_task = None # для корректного завершения работы\n",
    "        self._sem = asyncio.Semaphore(max_rate)\n",
    "        self._cuncurrent_workers = 0\n",
    "        self._stop_event = asyncio.Event()\n",
    "\n",
    "    async def _worker(self, task):\n",
    "        async with self._sem:\n",
    "            self._cuncurrent_workers += 1\n",
    "            await task.perform()\n",
    "            self.queue.task_done()\n",
    "        self._cuncurrent_workers -= 1\n",
    "        print(self._cuncurrent_workers)\n",
    "        if not self.is_running and self._cuncurrent_workers == 0:\n",
    "            self._stop_event.set()\n",
    "\n",
    "    async def _scheduler(self):\n",
    "        \"\"\"\n",
    "        планировщик scheduler, который работает постоянно,\n",
    "        просыпается раз в объявленный интервал, достает из\n",
    "        очереди max_rate задач и запускает их исполнение\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            for _ in range(self.max_rate):\n",
    "                async with self._sem:\n",
    "                    task = await self.queue.get()\n",
    "                    asyncio.create_task(self._worker(task))\n",
    "            await asyncio.sleep(self.interval)\n",
    "\n",
    "    def start(self):\n",
    "        # Запускает задачи\n",
    "        self.is_running = True\n",
    "        self._scheduler_task = asyncio.create_task(self._scheduler())\n",
    "\n",
    "    async def stop(self):\n",
    "        # Останавливает работу\n",
    "        self.is_running = False\n",
    "        self._scheduler_task.cancel()\n",
    "        if self._cuncurrent_workers != 0:\n",
    "            await self._stop_event.wait()\n",
    "\n",
    "\n",
    "def get_article_urls(last_news):\n",
    "    \"\"\"\n",
    "    Функция получения ссылок на \"свежие\" новости\n",
    "\n",
    "    :param last_news: id последней скачанной новости\n",
    "    :return: data list of tuple, состоящих из id и url новости\n",
    "            last_id - id последней скачанной новости\n",
    "    \"\"\"\n",
    "    resp = requests.get(URL)\n",
    "    data =  resp.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    all_news = (news.a['href'] if 'item?' not in news.a['href'] else ''\n",
    "                for news in soup.findAll('span', class_='titleline'))\n",
    "    id_news = (news['id'] for news in soup.findAll('tr', class_='athing'))\n",
    "    data = []\n",
    "    for id_news, link in zip(id_news, all_news):\n",
    "        if id_news == last_news:\n",
    "            break\n",
    "        data.append((id_news, link))\n",
    "    last_id = data[0][0] if data else last_news\n",
    "    logging.info('Handle main page: {} new articles'.format(len(data)))\n",
    "    return data, last_id\n",
    "\n",
    "\n",
    "async def pump_over_links(news_id):\n",
    "    \"\"\"\n",
    "    Функция получения ссылок из комментариев новости\n",
    "\n",
    "    :param id: id новости\n",
    "    :return: list of ссылок из комментариев\n",
    "    \"\"\"\n",
    "    url = URL+'/item?id='+news_id\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as resp:\n",
    "            logging.debug('Pump over comments for {}'.format(url))\n",
    "            data = await resp.text()\n",
    "            root = etree.HTML(data)\n",
    "            all_href = [(news_id, href.get('href')) for href in root.xpath('//div[@class=\"comment\"]//a[@rel=\"nofollow\"]')]\n",
    "            return all_href\n",
    "\n",
    "\n",
    "async def start(pool, result_dir, interval):\n",
    "    \"\"\"\n",
    "    Функция запуска краулера\n",
    "\n",
    "    :param pool: объект класса Pool\n",
    "            (сущность умеет управлять количеством запросов в единицу времени)\n",
    "    :param result_dir: апка для хранения данных\n",
    "    :param interval: Интервал запуска обкачки страниц\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    last_id = 0\n",
    "    while True:\n",
    "        pool.start()\n",
    "        # получаем url статей с основной страницы\n",
    "        links, last_id = get_article_urls(last_id)\n",
    "        for link in links:\n",
    "            # кладем в очередь обкачки каждую статью\n",
    "            await pool.queue.put(FetchTask(link, result_dir))\n",
    "            # получаем url из комментариев на комментарии\n",
    "            sub_links = await pump_over_links(link[0])\n",
    "            for sub_link in sub_links:\n",
    "                # кладем в очередь обкачки каждую ссылку из комментария\n",
    "                await pool.queue.put(FetchTask(sub_link, result_dir))\n",
    "        await pool.queue.join()\n",
    "        await pool.stop()\n",
    "        # ждем установленное время для начала след итерации обкачки сайта\n",
    "        await asyncio.sleep(interval)\n",
    "\n",
    "\n",
    "def main(result_dir, interval, rate):\n",
    "    \"\"\"\n",
    "    Основная функция запуска цикла событий\n",
    "\n",
    "    :param result_dir: Папка для хранения данных\n",
    "    :param interval: Интервал запуска обкачки страниц\n",
    "    :param rate: Максимальное кол-во запросов в (HANDLE_INTERVAL сек) для предотвращения блокировок\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    pool = Pool(rate)\n",
    "    try:\n",
    "        logging.info('Crawler started')\n",
    "        loop.run_until_complete(start(pool, result_dir, interval))\n",
    "    # Обработка принудительной остановки краулера\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info('Crawler stopped')\n",
    "        loop.run_until_complete(pool.stop())\n",
    "        loop.close()\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"\n",
    "    Функция парсинга аргументов\n",
    "\n",
    "    :return: argparse.Namespace: Program arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Async crawler for news.ycombinator.com (YCrawler)'\n",
    "    )\n",
    "    parser.add_argument('-o', '--output',  default=DOCUMENT_ROOT)\n",
    "    parser.add_argument('-i', '--interval', default=CHECK_INTERVAL)\n",
    "    parser.add_argument('-d', '--debug', action='store_true')\n",
    "    parser.add_argument('-r', '--rate', default=MAX_RATE)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_arguments()\n",
    "    # Создаем папку для хранения страниц\n",
    "    result_dir = str(pathlib.Path(args.output))\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    # Настраиваем логгирование\n",
    "    logging.basicConfig(level=logging.DEBUG if args.debug else logging.INFO,\n",
    "                        format='[%(asctime)s] %(levelname).1s %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    main(result_dir, args.interval, args.rate)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
